# EngineerOptimization

## 现代优化算法

### 遗传算法(genetic algorithms)





### 模拟退火算法(simulated annealing)

​		模拟退火算法是20世纪80年代发展起来的一种用于求解大规模组合优化问题的随机寻优算法。

#### 1.基本原理

​		模拟退火算法以模拟高温固体的热退火为基础。高温时原子会保持自由的无规律热运动，在温度降低时热运动减弱，原子会趋近于有序态，并最终形成晶体，冷却速度会影响这一过程的晶体形成。冷却速度过快可能会形成更高能态的多晶态。

​		通过引用Boltzmann概率分布来模拟冷却过程，引入类能量参数进行控制。

​		$P(E) = e^{-E/kT}$	

​		式中$P(E)$为处于能量$E$的概率，可以看出温度较低时，系统处在高能状态的可能性较小，这表明，当假设搜索过程遵循玻尔兹曼概率分布时，可以通过控制温度T来控制模拟退火算法的收敛性。

​		以目标函数求最小值为例，能量由设计状态的目标函数值决定，$E_i=f_i=f\left(\mathbf{X}_i\right)$。下一步设计状态$\mathbf{X_{i+1}}$的的概率由其与当前设计状态的能量差决定，这一选取准则被称为**Metropolis准则**，可用下式描述：

​		$P[E_{i+1}]=\min\left\{1,e^{-\Delta E/kT}\right\}$

​		为了简便运算，可取$k=1$；同时观察到当$\Delta E \leq 0$，恒有$P(E_{i+1})=1$，因此一定接受这些新的设计状态。同时，对于那些$\Delta E > 0$，按照准则也有一定接受的概率，这也是全局寻优的特点。同时我们注意到，当“温度”较高时，即使是很糟糕的的设计状态$\mathbf{X_{i+1}}$也有相对较高的可能性接受，而在"温度"较低时，对于能量较高的设计状态$\mathbf{X_{i+1}}$将会有更小的概率被接受，从而令过程向最优解趋近。

#### 2.算法步骤

​		首先定义一个初始温度$T_0$与初始设计向量$\mathbf{X_{1}}$，在其邻域内随机生产新的设计向量，按照**Metropolis准则**，计算$\Delta E$:为负则接受，为正则按照概率决定接受与否；至此完成了SA算法的一次迭代。若拒绝该设计向量则随机生成一个新的设计向量$\mathbf{X_{i+1}}$，继续迭代；

​		在每个温度下进行多次设计状态的迭代搜索，即为**迭代数n**；

`在实际应用中，迭代数n的选择通常需要根据问题规模和要求的解精度来确定。一般情况下，n的取值越大，算法搜索的状态空间就越大，因此更有可能找到全局最优解，但同时也会增加算法的计算时间和计算资源消耗。`

​		若迭代的数目超过n，则利用**降温系数c**进行降温操作，并复位$i=1$进行新温度下的搜索。降温策略可以使用：1.等比例降温 $T^{k+1}=cT^k$ 	2.等长度降温 $T^{k+1}=\dfrac{T^0\left(N-k\right)}{N}$ 

​		当温度足够低或$\Delta f$足够小时，则认为SA达到收敛。收敛准则可以选取：1. 零度法 $T^k\leq\epsilon_T$	2. 循环数控制 $k > k_{max}$​	3.基于不改进的控制：在给定的温度和迭代步内，局部最优没有改进的累计步数

#### 3.特点

- 最优解不受初始状态影响，但会影响计算量。
- 全局搜索
- 具有通用性，无需梯度
- 收敛特性不受目标函数的连续性和凸/凹特性影响
- 对设计变量无需编码，也无需是正数
- 设计行为约束的问题，需要表示为一个等价的无约束函数
- 退火降温速率慢，优化时间长



### 粒子群算法

#### 利用scikit-opt库的python实现

| 入参           | 默认值 | 意义             |
| -------------- | ------ | ---------------- |
| func           | -      | 目标函数         |
| n_dim          | -      | 目标函数的维度   |
| size_pop       | 50     | 种群规模         |
| max_iter       | 200    | 最大迭代次数     |
| lb             | None   | 每个参数的最小值 |
| ub             | None   | 每个参数的最大值 |
| w              | 0.8    | 惯性权重         |
| c1             | 0.5    | 个体记忆         |
| c2             | 0.5    | 集体记忆         |
| constraint_ueq | 空元组 | 不等式约束       |

输出参数

- `pso.record_value` 每一代的粒子位置、粒子速度、对应的函数值。`pso.record_mode = True` 才开启记录
- `pso.gbest_y_hist` 历史最优函数值
- `pso.best_y` 最优函数值 （迭代中使用的是 `pso.gbest_x`, `pso.gbest_y`）
- `pso.best_x` 最优函数值对应的输入值



$v_{ij}(t + 1) = w * v_{ij}(t) + c_{p}r_{1j}(t)[y_{ij}(t) − x_{ij}(t)]+ c_{g}r_{2j}(t)[\hat{y}_{j}(t) − x_{ij}(t)]$ 	

优化策略：

- 使用非对称的学习因子，使得c1不断减小，c2不断增加
- 使用随迭代次数递减的惯性因子
- $\omega_{id}=\text{sigmod}\left(\alpha.\frac{\Delta h}{\nu_{id}}\right)$



